{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d62126d0-2fb7-4868-9e2f-d02ac56d21b4",
   "metadata": {},
   "source": [
    "NLP Challenge: IMDB Dataset of 50K Movie Reviews to perform Sentiment analysis\n",
    "https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473c01ed-960b-4467-a874-7116565a6f33",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cdee197-b9cb-4e79-9d67-1b7b2a2eea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.68.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ichwi\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4b3c9d-6950-4146-ae82-d45c27316d03",
   "metadata": {},
   "source": [
    "Load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c69438f2-a424-4d1f-8f82-e66ae2f1c8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = './imdb50kreviews/IMDB Dataset.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff59e1f4-9e01-406e-b633-831a6b41999a",
   "metadata": {},
   "source": [
    "Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c86e122c-fbd0-419d-b63b-a38f8e33cfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['review', 'sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bc6132d-6301-4ecf-8c5a-b5a68a2279ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset contains 50000 examples\n",
      "\n",
      "Text of second example look like this: A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.\n",
      "\n",
      "Labels of first 5 examples look like this: [1, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "#create data set, classifying labels to 0 for negative and 1 for positive\n",
    "sentences = df['review'].to_numpy()\n",
    "labels = df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1).to_numpy()\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sentences, labels))\n",
    "\n",
    "#take a look\n",
    "examples = list(dataset.take(5))\n",
    "\n",
    "print(f\"dataset contains {len(dataset)} examples\\n\")\n",
    "\n",
    "print(f\"Text of second example look like this: {examples[1][0].numpy().decode('utf-8')}\\n\")\n",
    "print(f\"Labels of first 5 examples look like this: {[x[1].numpy() for x in examples]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a3aa3-ad0f-45b6-8517-bc2fd43668fa",
   "metadata": {},
   "source": [
    "Split to training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d0e7bce0-c50a-4e1c-9385-1c495904c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SPLIT = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "71d15bc0-8ec6-4fdb-8386-f03d4eaa8bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 45000 elements for training.\n",
      "\n",
      "There are 5000 elements for validation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(dataset) * TRAINING_SPLIT)\n",
    "\n",
    "train_dataset = dataset.take(train_size)\n",
    "validation_dataset = dataset.skip(train_size)\n",
    "\n",
    "print(f\"There are {len(train_dataset)} elements for training.\\n\")\n",
    "print(f\"There are {len(validation_dataset)} elements for validation.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163e7c5a-e311-493b-9ca2-1feb3865572f",
   "metadata": {},
   "source": [
    "Vectorization and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12e2b24f-3f55-45f0-ad6e-af60bfe0a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "229b7a45-f00e-485a-861f-e0b2b6edf73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 172364 words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create and adapt the vectorizer\n",
    "vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    output_sequence_length=MAX_LENGTH,\n",
    "    output_mode='int'\n",
    ")\n",
    "\n",
    "vectorizer.adapt(train_dataset.map(lambda x, y: x))\n",
    "\n",
    "#Get the vocabulary size after adaptation\n",
    "vocab_size = len(vectorizer.get_vocabulary())\n",
    "\n",
    "print(f\"Vocabulary contains {vocab_size} words\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "95d6a118-0d0e-4840-8c8c-eac1dbdc1f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply vectorization and padding to train and validation datasets\n",
    "\n",
    "def vectorize_and_pad(x, y):\n",
    "    #Vectorize the input\n",
    "    vectorized = vectorizer(x)\n",
    "    #Pad the vectorized sequence\n",
    "    padded = tf.pad(vectorized, [[0, MAX_LENGTH - tf.shape(vectorized)[0]]], constant_values=0)\n",
    "    #Ensure the padded sequence has the correct shape\n",
    "    padded = tf.ensure_shape(padded, [MAX_LENGTH])\n",
    "    return padded, y\n",
    "\n",
    "#Apply vectorization and padding\n",
    "train_dataset_vectorized = train_dataset.map(vectorize_and_pad)\n",
    "val_dataset_vectorized = validation_dataset.map(vectorize_and_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb41d61c-31ab-4942-bf3a-307e76df8b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(120,), dtype=int64, numpy=\n",
      "array([   29,     5,     2,    78,  1948,    45,  1060,    12,   100,\n",
      "         146,    41,   482,  3199,   397,   457,    27,  3220,    35,\n",
      "          24,   204,    15,    11,     7,   600,    49,   591,    16,\n",
      "        2112,    13,     2,    88,   148,    12,  3288,    70,    43,\n",
      "        3199,    14,    30,  5687,     3, 14712,   135,     5,   593,\n",
      "          61,   281,     8,   204,    36,     2,   680,   139,  1688,\n",
      "          70,    11,     7,    22,     4,   119,    17,     2,  8756,\n",
      "        5821,    40, 11585,    11,   119,  2413,    56,  5961,    16,\n",
      "        5557,     6,  1465,   384,    40,   593,    30,     7,  3460,\n",
      "           8,     2,   352,   342,     5,     2, 22149,    13,     9,\n",
      "           7,   469,  3199,    15,    12,     7,     2, 11311,   344,\n",
      "           6,     2, 15503,  6852,  2569,  1074, 65347,     9,  2626,\n",
      "        1386,    21, 25866,   536,    34,  4883,  2469,     5,     2,\n",
      "        1185,   114,    32], dtype=int64)>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      "\n",
      "(<tf.Tensor: shape=(120,), dtype=int64, numpy=\n",
      "array([     4,    385,    117,    358,     13,     13,      2,   1353,\n",
      "         2962,      7,     53,  19358,     53, 112817,   1647,      3,\n",
      "          394,      4,  12979,      3,    519,  28711,    284,      5,\n",
      "         1873,      6,      2,    426,    405,     13,     13,      2,\n",
      "          150,     24,    544,     74,   2259,    496,   4524,     22,\n",
      "           62,     45,    186,     32,      2, 108042,     19,     28,\n",
      "           45,     32,      2,   2199,    183,   3346,    101,     23,\n",
      "           69,    353,     65,      2,  14443,    799,  10280,     33,\n",
      "            2,   1823,      6,   1686,   7528,   6759,     22,     62,\n",
      "            7,      9,     74,    267,      2,    146,     19,      9,\n",
      "            7,      4,  39556,    431,      3,   2365,    405,      4,\n",
      "         4450,    358,     43,     29,      5,      2,     80,   3217,\n",
      "            5,    212,      3,     25,    120,     13,     13,      2,\n",
      "         1873,     63,    263,    339,     16,      2,    117,    179,\n",
      "            2,   1085,      5,      2,   2914,     61,    242,     71],\n",
      "      dtype=int64)>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Test view 2 training sequences and their labels\n",
    "for example in train_dataset_vectorized.take(2):\n",
    "    print(example)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29eb56c2-9835-4ad4-8974-31aff563dd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimize and batch datasets for training\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "PREFETCH_BUFFER_SIZE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "be8b30b2-d65a-47f0-8ff7-27fdb7fef463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (128, 120)\n",
      "Label shape: (128,)\n"
     ]
    }
   ],
   "source": [
    "train_dataset_final = (train_dataset_vectorized\n",
    "                       .cache()\n",
    "                       .shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "                       .prefetch(PREFETCH_BUFFER_SIZE)\n",
    "                       .batch(BATCH_SIZE)\n",
    "                      )\n",
    "\n",
    "val_dataset_final = (val_dataset_vectorized\n",
    "                     .cache()\n",
    "                     .prefetch(PREFETCH_BUFFER_SIZE)\n",
    "                     .batch(BATCH_SIZE)\n",
    "                    )\n",
    "\n",
    "for batch in train_dataset_final.take(1):\n",
    "    print(\"Input shape:\", batch[0].shape)\n",
    "    print(\"Label shape:\", batch[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5dc90b-cc78-4dc9-924d-76b3700eff38",
   "metadata": {},
   "source": [
    "Model LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78410baa-8c9f-40f3-8b37-af972d75cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 16\n",
    "LSTM_DIM = 32\n",
    "DENSE_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ac28e1fa-3ccc-413f-96ca-50e64c4361f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,757,824</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │     \u001b[38;5;34m2,757,824\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m12,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m390\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m7\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,770,765</span> (10.57 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,770,765\u001b[0m (10.57 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,770,765</span> (10.57 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,770,765\u001b[0m (10.57 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(MAX_LENGTH,)),\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_DIM, kernel_regularizer=tf.keras.regularizers.L2(0.01))),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(DENSE_DIM, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "800f900a-d434-4eb0-8f51-fc19c6b662af",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "561dedd8-04b7-49ec-a616-8e4537f106b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 85ms/step - accuracy: 0.6354 - loss: 0.8503 - val_accuracy: 0.8434 - val_loss: 0.3792 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8762 - loss: 0.3119\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 84ms/step - accuracy: 0.8762 - loss: 0.3118 - val_accuracy: 0.8510 - val_loss: 0.3805 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9391 - loss: 0.1759\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 86ms/step - accuracy: 0.9391 - loss: 0.1758 - val_accuracy: 0.8580 - val_loss: 0.3799 - learning_rate: 2.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9549 - loss: 0.1414\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 87ms/step - accuracy: 0.9549 - loss: 0.1413 - val_accuracy: 0.8520 - val_loss: 0.4201 - learning_rate: 4.0000e-05\n",
      "Epoch 5/20\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 88ms/step - accuracy: 0.9570 - loss: 0.1360 - val_accuracy: 0.8506 - val_loss: 0.4394 - learning_rate: 1.0000e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 88ms/step - accuracy: 0.9576 - loss: 0.1347 - val_accuracy: 0.8506 - val_loss: 0.4421 - learning_rate: 1.0000e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.9578 - loss: 0.1320 - val_accuracy: 0.8502 - val_loss: 0.4434 - learning_rate: 1.0000e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.9596 - loss: 0.1305 - val_accuracy: 0.8506 - val_loss: 0.4464 - learning_rate: 1.0000e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 89ms/step - accuracy: 0.9600 - loss: 0.1280 - val_accuracy: 0.8508 - val_loss: 0.4477 - learning_rate: 1.0000e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 91ms/step - accuracy: 0.9610 - loss: 0.1269 - val_accuracy: 0.8510 - val_loss: 0.4513 - learning_rate: 1.0000e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 99ms/step - accuracy: 0.9614 - loss: 0.1256 - val_accuracy: 0.8506 - val_loss: 0.4514 - learning_rate: 1.0000e-05\n",
      "Epoch 12/20\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 94ms/step - accuracy: 0.9618 - loss: 0.1236 - val_accuracy: 0.8500 - val_loss: 0.4547 - learning_rate: 1.0000e-05\n",
      "Epoch 13/20\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 91ms/step - accuracy: 0.9630 - loss: 0.1215 - val_accuracy: 0.8504 - val_loss: 0.4559 - learning_rate: 1.0000e-05\n",
      "Epoch 14/20\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 90ms/step - accuracy: 0.9637 - loss: 0.1200 - val_accuracy: 0.8496 - val_loss: 0.4614 - learning_rate: 1.0000e-05\n",
      "Epoch 15/20\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 89ms/step - accuracy: 0.9636 - loss: 0.1195 - val_accuracy: 0.8490 - val_loss: 0.4653 - learning_rate: 1.0000e-05\n",
      "Epoch 16/20\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 91ms/step - accuracy: 0.9645 - loss: 0.1181 - val_accuracy: 0.8498 - val_loss: 0.4622 - learning_rate: 1.0000e-05\n",
      "Epoch 17/20\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 89ms/step - accuracy: 0.9650 - loss: 0.1158 - val_accuracy: 0.8492 - val_loss: 0.4641 - learning_rate: 1.0000e-05\n",
      "Epoch 18/20\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 96ms/step - accuracy: 0.9663 - loss: 0.1131 - val_accuracy: 0.8482 - val_loss: 0.4716 - learning_rate: 1.0000e-05\n",
      "Epoch 19/20\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 89ms/step - accuracy: 0.9666 - loss: 0.1129 - val_accuracy: 0.8480 - val_loss: 0.4759 - learning_rate: 1.0000e-05\n",
      "Epoch 20/20\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 89ms/step - accuracy: 0.9669 - loss: 0.1107 - val_accuracy: 0.8482 - val_loss: 0.4729 - learning_rate: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "history = model.fit(train_dataset_final,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=val_dataset_final,\n",
    "                    callbacks=[ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                 factor=0.2, verbose=1,\n",
    "                                                 patience=1, min_lr=0.00001)\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671ccc1e-c843-4b6c-83cd-ece356b82248",
   "metadata": {},
   "source": [
    "Test the Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "be528990-6f57-4d28-ae54-924a3ad57a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 412ms/step\n",
      "Sentence: This movie was absolutely fantastic! I loved every minute of it.\n",
      "Predicted sentiment: Positive\n",
      "Confidence: 0.89\n"
     ]
    }
   ],
   "source": [
    "#Save the model\n",
    "model.save('sentiment_model.keras')\n",
    "\n",
    "def predict_sentiment(sentence):\n",
    "    #Preprocess the sentence\n",
    "    vectorized_sentence = vectorizer([sentence])\n",
    "    #Make prediction\n",
    "    prediction = model.predict(vectorized_sentence)\n",
    "    #Interpret the result\n",
    "    sentiment = \"Positive\" if prediction[0][0] > 0.5 else \"Negative\"\n",
    "    confidence = prediction[0][0] if sentiment == \"Positive\" else 1-prediction[0][0]\n",
    "    return sentiment, confidence\n",
    "\n",
    "#Test the function\n",
    "test_sentence = \"This movie was absolutely fantastic! I loved every minute of it.\"\n",
    "sentiment, confidence = predict_sentiment(test_sentence)\n",
    "print(f\"Sentence: {test_sentence}\")\n",
    "print(f\"Predicted sentiment: {sentiment}\")\n",
    "print(f\"Confidence: {confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bf426d4d-5d64-49e6-8c0c-1b16de84ea8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Sentence: I hated this film, it was a complete waste of time.\n",
      "Predicted sentiment: Negative\n",
      "Confidence: 0.91\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Sentence: The actors were marvelous, but the plot was a disaster.\n",
      "Predicted sentiment: Positive\n",
      "Confidence: 0.59\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Sentence: A masterpiece! One of the most memorable film I've ever seen.\n",
      "Predicted sentiment: Positive\n",
      "Confidence: 0.94\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Sentence: The movie was well-received by critics but I didn't find it very interesting.\n",
      "Predicted sentiment: Positive\n",
      "Confidence: 0.79\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Sentence: It was okay, I didn't think much about. I forgot about it pretty quickly.\n",
      "Predicted sentiment: Positive\n",
      "Confidence: 0.60\n"
     ]
    }
   ],
   "source": [
    "#Test more sentence\n",
    "test_sentences = [\n",
    "    \"I hated this film, it was a complete waste of time.\",\n",
    "    \"The actors were marvelous, but the plot was a disaster.\",\n",
    "    \"A masterpiece! One of the most memorable film I've ever seen.\",\n",
    "    \"The movie was well-received by critics but I didn't find it very interesting.\",\n",
    "    \"It was okay, I didn't think much about. I forgot about it pretty quickly.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    sentiment, confidence = predict_sentiment(sentence)\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Predicted sentiment: {sentiment}\")\n",
    "    print(f\"Confidence: {confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b1db0d-fe3f-4357-be1a-3b3746605909",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
